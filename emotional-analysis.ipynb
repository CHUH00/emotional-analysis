{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2ba734",
   "metadata": {},
   "source": [
    "# 감성 분석 모델 학습 및 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd545eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62585e3a",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a97edcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>공포</th>\n",
       "      <th>5468</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>놀람</td>\n",
       "      <td>5898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>분노</td>\n",
       "      <td>5665.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>슬픔</td>\n",
       "      <td>5267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>중립</td>\n",
       "      <td>4830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>행복</td>\n",
       "      <td>6037.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sentence Emotion  Unnamed: 2  Unnamed: 3  Unnamed: 4  공포  \\\n",
       "0  언니 동생으로 부르는게 맞는 일인가요..??      공포         NaN         NaN         NaN  놀람   \n",
       "1              그냥 내 느낌일뿐겠지?      공포         NaN         NaN         NaN  분노   \n",
       "2            아직너무초기라서 그런거죠?      공포         NaN         NaN         NaN  슬픔   \n",
       "3             유치원버스 사고 낫다던데      공포         NaN         NaN         NaN  중립   \n",
       "4               근데 원래이런거맞나요      공포         NaN         NaN         NaN  행복   \n",
       "\n",
       "     5468  \n",
       "0  5898.0  \n",
       "1  5665.0  \n",
       "2  5267.0  \n",
       "3  4830.0  \n",
       "4  6037.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communication_df = pd.read_excel('/Users/woojin/Desktop/SK Networks Family AI Camp_17/SKN-17-Family-AI-Camp/NATURAL_LANGUAGE_PROCESSING/04_dl_nlp_basic/한국어_단발성_대화_데이터셋.xlsx')\n",
    "communication_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1b739",
   "metadata": {},
   "source": [
    "### 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbd1aef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sentence Emotion\n",
       "0  언니 동생으로 부르는게 맞는 일인가요..??      공포\n",
       "1              그냥 내 느낌일뿐겠지?      공포\n",
       "2            아직너무초기라서 그런거죠?      공포\n",
       "3             유치원버스 사고 낫다던데      공포\n",
       "4               근데 원래이런거맞나요      공포"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communication_df = communication_df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', '공포', 5468], axis=1).reset_index(drop=True)\n",
    "communication_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f60ce6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence    0\n",
       "Emotion     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communication_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5b76452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "행복    6037\n",
       "놀람    5898\n",
       "분노    5665\n",
       "공포    5468\n",
       "혐오    5429\n",
       "슬픔    5267\n",
       "중립    4830\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communication_df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93ec8e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38594/38594 [00:38<00:00, 1013.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[언니, 동생, 부르다, 맞다, 인가요]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[그냥, 느낌, 겠다]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[아직, 너무, 초기, 라서, 그런]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[유치원, 버스, 사고, 낫다]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[근데, 원래, 이렇다, 맞다]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38589</th>\n",
       "      <td>[솔직하다, 예보, 제대로, 세금, 이라도, 아끼다, 그냥, 폐지]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38590</th>\n",
       "      <td>[재미, 없다, 망하다]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38591</th>\n",
       "      <td>[공장, 도시락, 비우다, 적임, 아르바이트, 화장실, 가성, 않씯, 재료, 담다,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38592</th>\n",
       "      <td>[코딱지, 나라, 지다, 들다, 끼리, 피터지다, 싸우다, 세다, 클래스, ㅉㅉㅉ]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38593</th>\n",
       "      <td>[와이프, 그렇다, 댓글, 보다, 이휘재, 하차, 하라, 해주다]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38594 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  Emotion\n",
       "0                                 [언니, 동생, 부르다, 맞다, 인가요]        3\n",
       "1                                           [그냥, 느낌, 겠다]        3\n",
       "2                                   [아직, 너무, 초기, 라서, 그런]        3\n",
       "3                                      [유치원, 버스, 사고, 낫다]        3\n",
       "4                                      [근데, 원래, 이렇다, 맞다]        3\n",
       "...                                                  ...      ...\n",
       "38589              [솔직하다, 예보, 제대로, 세금, 이라도, 아끼다, 그냥, 폐지]        4\n",
       "38590                                      [재미, 없다, 망하다]        4\n",
       "38591  [공장, 도시락, 비우다, 적임, 아르바이트, 화장실, 가성, 않씯, 재료, 담다,...        4\n",
       "38592     [코딱지, 나라, 지다, 들다, 끼리, 피터지다, 싸우다, 세다, 클래스, ㅉㅉㅉ]        4\n",
       "38593               [와이프, 그렇다, 댓글, 보다, 이휘재, 하차, 하라, 해주다]        4\n",
       "\n",
       "[38594 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "stopwords = set([\n",
    "    \"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"도\", \"로\", \"으로\", \"그리고\", \"하지만\", \"또는\", \"에서\", \"하다\"\n",
    "])\n",
    "\n",
    "def preprocess_korean(text: str, use_stem=True):\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\s]\", \" \", str(text))\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    tokens = okt.morphs(text, stem=use_stem)\n",
    "    \n",
    "    tokens = [tok for tok in tokens if tok not in stopwords and len(tok) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "preprocessed_data = []\n",
    "for idx, sentence in enumerate(tqdm(communication_df['Sentence'])):\n",
    "    communication_df['Sentence'][idx] = preprocess_korean(sentence)\n",
    "\n",
    "labels_str = ['행복', '놀람', '분노', '공포', '혐오', '슬픔', '중립']\n",
    "label_to_int = {label: i for i, label in enumerate(labels_str)}\n",
    "\n",
    "communication_df['Emotion'] = communication_df['Emotion'].map(label_to_int)\n",
    "\n",
    "communication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb495e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = communication_df['Sentence']\n",
    "y = communication_df['Emotion']\n",
    "\n",
    "w2v_size = 200\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=w2v_size,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=1,\n",
    "    epochs=10    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f263303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab = {w:i+1 for i,w in enumerate(w2v.wv.index_to_key)}  # 0=PAD\n",
    "unk = len(vocab)+1\n",
    "\n",
    "def to_ids(tokens): return [vocab.get(t, unk) for t in tokens]\n",
    "X_ids = [to_ids(toks) for toks in sentences]\n",
    "max_len = int(np.percentile([len(x) for x in X_ids], 95))\n",
    "X = pad_sequences(X_ids, maxlen=max_len, padding='post', truncating='post', value=0)\n",
    "y_np = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb9d5784",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = np.zeros((len(vocab)+2, w2v_size))\n",
    "for w,i in vocab.items():\n",
    "    emb[i] = w2v.wv[w]\n",
    "emb[unk] = emb[1:len(vocab)+1].mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53169547",
   "metadata": {},
   "source": [
    "### 3. 모델 정의 및 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e031e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,241,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">336,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">455</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m200\u001b[0m)        │     \u001b[38;5;34m2,241,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m336,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m455\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,595,399</span> (9.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,595,399\u001b[0m (9.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">353,799</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m353,799\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,241,600</span> (8.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,241,600\u001b[0m (8.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, layers, models\n",
    "\n",
    "num_classes = int(len(np.unique(y_np)))\n",
    "\n",
    "model = models.Sequential([\n",
    "    Input(shape=(max_len,)),\n",
    "    layers.Embedding(input_dim=emb.shape[0], output_dim=emb.shape[1],\n",
    "                     weights=[emb], mask_zero=False, trainable=False),\n",
    "    layers.Bidirectional(layers.LSTM(128)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ae6d0",
   "metadata": {},
   "source": [
    "### 4. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dff51d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.3039 - loss: 1.7266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 68ms/step - accuracy: 0.3368 - loss: 1.6628 - val_accuracy: 0.3880 - val_loss: 1.5917\n",
      "Epoch 2/15\n",
      "\u001b[1m244/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.3601 - loss: 1.6078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.3655 - loss: 1.5938 - val_accuracy: 0.3964 - val_loss: 1.5536\n",
      "Epoch 3/15\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.3783 - loss: 1.5736 - val_accuracy: 0.3949 - val_loss: 1.5447\n",
      "Epoch 4/15\n",
      "\u001b[1m244/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.3867 - loss: 1.5503"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.3892 - loss: 1.5484 - val_accuracy: 0.4102 - val_loss: 1.5373\n",
      "Epoch 5/15\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.3892 - loss: 1.5407 - val_accuracy: 0.3978 - val_loss: 1.5465\n",
      "Epoch 6/15\n",
      "\u001b[1m244/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.4012 - loss: 1.5247"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.3964 - loss: 1.5293 - val_accuracy: 0.4131 - val_loss: 1.5136\n",
      "Epoch 7/15\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.4044 - loss: 1.5146 - val_accuracy: 0.4047 - val_loss: 1.5155\n",
      "Epoch 8/15\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.4070 - loss: 1.5111 - val_accuracy: 0.4056 - val_loss: 1.5059\n",
      "Epoch 9/15\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.4112 - loss: 1.5004 - val_accuracy: 0.4088 - val_loss: 1.5114\n",
      "Test Acc: 0.3995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y_np, test_size=0.1, random_state=42, stratify=y_np\n",
    ")\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "ckpt = ModelCheckpoint('best_lstm.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_split=0.1,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    callbacks=[es, ckpt],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_te, y_te, verbose=0)\n",
    "print(f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b94dc1",
   "metadata": {},
   "source": [
    "### 5. 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc9ebc",
   "metadata": {},
   "source": [
    "Word2Vec 학습 (선택)\n",
    "\t•\tgensim으로 네 토큰 리스트 전체에 대해 Word2Vec 학습 → 임베딩 확인.\n",
    "\t•\t이걸 RNN/LSTM 분류기 입력으로 사용 가능. 이 방법으로 하고 싶은데 현재 내 파일 상황 보여줄게. 그 다음 어떤 걸 해야 되는 지 분석해서 알려줘."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab911548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tokens(tokens):\n",
    "    ids = [vocab.get(t, unk) for t in tokens]\n",
    "    arr = pad_sequences([ids], maxlen=max_len, padding='post', truncating='post', value=0)\n",
    "    \n",
    "    int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "    \n",
    "    probs = model.predict(arr, verbose=0)[0]\n",
    "    pred_class = int(np.argmax(probs))\n",
    "    confidence = float(np.max(probs))\n",
    "    label = int_to_label[pred_class]\n",
    "    \n",
    "    return label, pred_class, confidence, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e7155f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 토큰: ['오늘', '날씨', '좋다', '기분', '좋다']\n",
      "예측 클래스: 행복\n",
      "신뢰도: 0.9695\n",
      "전체 확률분포: [0.9694503  0.00314005 0.001149   0.00455268 0.00260659 0.00723143\n",
      " 0.01186989]\n"
     ]
    }
   ],
   "source": [
    "example = [\"오늘\", \"날씨\", \"좋다\", \"기분\", \"좋다\"]\n",
    "label, pred, conf, probs = predict_tokens(example)\n",
    "\n",
    "print(\"입력 토큰:\", example)\n",
    "print(\"예측 클래스:\", label)\n",
    "print(\"신뢰도:\", f\"{conf:.4f}\")\n",
    "print(\"전체 확률분포:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "229d1e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 토큰: ['비', '많이', '와']\n",
      "예측 클래스: 공포\n",
      "신뢰도: 0.2471\n",
      "전체 확률분포: [0.08591814 0.23476201 0.05705829 0.24710582 0.06371237 0.16405419\n",
      " 0.14738919]\n"
     ]
    }
   ],
   "source": [
    "example = [\"비\", \"많이\", \"와\"]\n",
    "label, pred, conf, probs = predict_tokens(example)\n",
    "\n",
    "print(\"입력 토큰:\", example)\n",
    "print(\"예측 클래스:\", label)\n",
    "print(\"신뢰도:\", f\"{conf:.4f}\")\n",
    "print(\"전체 확률분포:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5752e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 토큰: ['다이어트', '실패', '돼지']\n",
      "예측 클래스: 분노\n",
      "신뢰도: 0.2190\n",
      "전체 확률분포: [0.11259221 0.17622636 0.21897945 0.09990779 0.15013286 0.0898891\n",
      " 0.15227222]\n"
     ]
    }
   ],
   "source": [
    "example = [\"다이어트\", \"실패\", \"돼지\"]\n",
    "label, pred, conf, probs = predict_tokens(example)\n",
    "\n",
    "print(\"입력 토큰:\", example)\n",
    "print(\"예측 클래스:\", label)\n",
    "print(\"신뢰도:\", f\"{conf:.4f}\")\n",
    "print(\"전체 확률분포:\", probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
